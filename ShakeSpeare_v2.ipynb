{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olfabre/amsProjetMaster1/blob/olivier/ShakeSpeare_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Version améliorée 1"
      ],
      "metadata": {
        "id": "EF58ijdqn8M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import unidecode\n",
        "except ModuleNotFoundError:\n",
        "    !pip install unidecode\n",
        "    import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Vérification du GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"CUDA AVAILABLE\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"ONLY CPU AVAILABLE\")\n",
        "\n",
        "# Paramètres globaux\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "chunk_len = 13\n",
        "\n",
        "n_epochs = 200000\n",
        "print_every = 10\n",
        "plot_every = 10\n",
        "hidden_size = 512\n",
        "n_layers = 3\n",
        "lr = 0.005\n",
        "\n",
        "# Téléchargement des données depuis une URL\n",
        "def download_data(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "# Chargement des données\n",
        "url = \"https://olivier-fabre.com/passwordgenius/shakespeare2.txt\"\n",
        "data_dir = \"data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "data_path = os.path.join(data_dir, \"shakespeare2.txt\")\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Téléchargement des données...\")\n",
        "    download_data(url, data_path)\n",
        "\n",
        "# Lecture et traitement du fichier\n",
        "file = unidecode.unidecode(open(data_path, \"r\", encoding=\"utf-8\").read())\n",
        "file_len = len(file)\n",
        "print(f\"Longueur du corpus : {file_len}\")\n",
        "\n",
        "# Fonctions de préparation des données\n",
        "def random_chunk(file):\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "def random_training_set(file):\n",
        "    chunk = random_chunk(file)\n",
        "    inp = char_tensor(chunk[:-1]).to(device)\n",
        "    target = char_tensor(chunk[1:]).to(device)\n",
        "    return inp, target\n",
        "\n",
        "# Définition du modèle\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size, device=device))\n",
        "\n",
        "# Fonctions d'entraînement et d'évaluation\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(inp.size(0)):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / chunk_len\n",
        "\n",
        "def training(n_epochs, file, chunk_count=10):\n",
        "    print()\n",
        "    print('-----------')\n",
        "    print('|  TRAIN  |')\n",
        "    print('-----------')\n",
        "    print()\n",
        "\n",
        "    start = time.time()\n",
        "    all_losses = []\n",
        "    loss_avg = 0    # Moyenne des pertes sur tout l'entraînement\n",
        "    best_loss = float(\"inf\")\n",
        "    print_every = n_epochs // 100\n",
        "    eval_every = n_epochs // 100\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        losses = []\n",
        "        for _ in range(chunk_count):\n",
        "            loss = train(*random_training_set(file))\n",
        "            losses.append(loss)\n",
        "\n",
        "        # Moyenne sur les chunks\n",
        "        loss_avg += sum(losses) / chunk_count\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print('[%s (%d %d%%) Perte moyenne: %.4f Dernière perte: %.4f]' % (\n",
        "                time_since(start), epoch, epoch / n_epochs * 100, loss_avg / epoch, losses[-1]))\n",
        "\n",
        "        if epoch % eval_every == 0:\n",
        "            print()\n",
        "            print(f\"Évaluation à l'epoch {epoch}:\")\n",
        "            print(evaluate(decoder, prime_str='Wh', predict_len=100, temperature=0.8))\n",
        "            print()\n",
        "\n",
        "        if best_loss > (loss_avg / epoch):\n",
        "            best_loss = loss_avg / epoch\n",
        "            print('[%s (%d %d%%) Nouvelle meilleure perte moyenne: %.4f]' % (\n",
        "                time_since(start), epoch, epoch / n_epochs * 100, best_loss))\n",
        "\n",
        "def evaluate(decoder, prime_str=\"A\", predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str).to(device)\n",
        "    predicted = prime_str\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char).to(device)\n",
        "    return predicted\n",
        "\n",
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{m}m {s:.2f}s'\n",
        "\n",
        "# Lancement principal\n",
        "if __name__ == \"__main__\":\n",
        "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Début de l'entraînement...\")\n",
        "    training(1000, file)  # Ajustez n_epochs pour vos besoins\n",
        "\n",
        "    print(\"\\nÉvaluation...\")\n",
        "    print(evaluate(decoder, prime_str=\"To be or not to be\", predict_len=200, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "WlaL_n-_ugv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4oBGI42pxld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}