{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olfabre/amsProjetMaster1/blob/olivier/ShakeSpeare_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version initiale"
      ],
      "metadata": {
        "id": "EF58ijdqn8M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import unidecode\n",
        "except ModuleNotFoundError:\n",
        "    !pip install unidecode\n",
        "    import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "# Vérification du GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"CUDA AVAILABLE\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"ONLY CPU AVAILABLE\")\n",
        "\n",
        "# Paramètres globaux\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "chunk_len = 13\n",
        "\n",
        "n_epochs = 200000\n",
        "print_every = 10\n",
        "plot_every = 10\n",
        "hidden_size = 512\n",
        "n_layers = 3\n",
        "lr = 0.005\n",
        "\n",
        "# Téléchargement des données depuis une URL\n",
        "def download_data(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "# Chargement des données\n",
        "url = \"https://olivier-fabre.com/passwordgenius/shakespeare2.txt\"\n",
        "data_dir = \"data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "data_path = os.path.join(data_dir, \"shakespeare2.txt\")\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Téléchargement des données...\")\n",
        "    download_data(url, data_path)\n",
        "\n",
        "# Lecture et traitement du fichier\n",
        "file = unidecode.unidecode(open(data_path, \"r\", encoding=\"utf-8\").read())\n",
        "file_len = len(file)\n",
        "print(f\"Longueur du corpus : {file_len}\")\n",
        "\n",
        "# Fonctions de préparation des données\n",
        "def random_chunk(file):\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "def random_training_set(file):\n",
        "    chunk = random_chunk(file)\n",
        "    inp = char_tensor(chunk[:-1]).to(device)\n",
        "    target = char_tensor(chunk[1:]).to(device)\n",
        "    return inp, target\n",
        "\n",
        "# Définition du modèle\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size, device=device))\n",
        "\n",
        "# Fonctions d'entraînement et d'évaluation\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(inp.size(0)):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / chunk_len\n",
        "\n",
        "def training(n_epochs, file):\n",
        "    start = time.time()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train(*random_training_set(file))\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"[{time_since(start)} ({epoch}/{n_epochs})] Perte : {loss:.4f}\")\n",
        "\n",
        "def evaluate(decoder, prime_str=\"A\", predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str).to(device)\n",
        "    predicted = prime_str\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char).to(device)\n",
        "    return predicted\n",
        "\n",
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{m}m {s:.2f}s'\n",
        "\n",
        "# Lancement principal\n",
        "if __name__ == \"__main__\":\n",
        "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Début de l'entraînement...\")\n",
        "    training(1000, file)  # Ajustez n_epochs pour vos besoins\n",
        "\n",
        "    print(\"\\nÉvaluation...\")\n",
        "    print(evaluate(decoder, prime_str=\"To be or not to be\", predict_len=200, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "jUMPDbbFoNFp",
        "outputId": "097f9652-c96e-47b0-95ce-0dff64b557b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA AVAILABLE\n",
            "Longueur du corpus : 314122\n",
            "Début de l'entraînement...\n",
            "[0m 0.37s (10/1000)] Perte : 4.1916\n",
            "[0m 0.75s (20/1000)] Perte : 6.0787\n",
            "[0m 1.11s (30/1000)] Perte : 4.0047\n",
            "[0m 1.48s (40/1000)] Perte : 3.0178\n",
            "[0m 1.89s (50/1000)] Perte : 3.6313\n",
            "[0m 2.28s (60/1000)] Perte : 3.4829\n",
            "[0m 2.55s (70/1000)] Perte : 3.4531\n",
            "[0m 2.80s (80/1000)] Perte : 3.3117\n",
            "[0m 3.05s (90/1000)] Perte : 3.5916\n",
            "[0m 3.29s (100/1000)] Perte : 4.2990\n",
            "[0m 3.52s (110/1000)] Perte : 3.5007\n",
            "[0m 3.78s (120/1000)] Perte : 2.8189\n",
            "[0m 4.02s (130/1000)] Perte : 4.8903\n",
            "[0m 4.25s (140/1000)] Perte : 3.2798\n",
            "[0m 4.49s (150/1000)] Perte : 4.0331\n",
            "[0m 4.72s (160/1000)] Perte : 3.0311\n",
            "[0m 4.97s (170/1000)] Perte : 3.0005\n",
            "[0m 5.19s (180/1000)] Perte : 3.1937\n",
            "[0m 5.43s (190/1000)] Perte : 3.1834\n",
            "[0m 5.68s (200/1000)] Perte : 3.1069\n",
            "[0m 5.93s (210/1000)] Perte : 2.8318\n",
            "[0m 6.17s (220/1000)] Perte : 2.9983\n",
            "[0m 6.41s (230/1000)] Perte : 3.0574\n",
            "[0m 6.64s (240/1000)] Perte : 3.4660\n",
            "[0m 6.89s (250/1000)] Perte : 3.8896\n",
            "[0m 7.13s (260/1000)] Perte : 2.7422\n",
            "[0m 7.37s (270/1000)] Perte : 2.9514\n",
            "[0m 7.60s (280/1000)] Perte : 2.7864\n",
            "[0m 7.84s (290/1000)] Perte : 3.0209\n",
            "[0m 8.08s (300/1000)] Perte : 2.8339\n",
            "[0m 8.32s (310/1000)] Perte : 2.9804\n",
            "[0m 8.55s (320/1000)] Perte : 3.8409\n",
            "[0m 8.79s (330/1000)] Perte : 2.9850\n",
            "[0m 9.04s (340/1000)] Perte : 2.9494\n",
            "[0m 9.27s (350/1000)] Perte : 3.4220\n",
            "[0m 9.50s (360/1000)] Perte : 3.3187\n",
            "[0m 9.74s (370/1000)] Perte : 2.9101\n",
            "[0m 9.98s (380/1000)] Perte : 2.5900\n",
            "[0m 10.22s (390/1000)] Perte : 2.9007\n",
            "[0m 10.46s (400/1000)] Perte : 3.1162\n",
            "[0m 10.69s (410/1000)] Perte : 2.6041\n",
            "[0m 10.93s (420/1000)] Perte : 2.8697\n",
            "[0m 11.17s (430/1000)] Perte : 2.6580\n",
            "[0m 11.40s (440/1000)] Perte : 2.7361\n",
            "[0m 11.64s (450/1000)] Perte : 2.3686\n",
            "[0m 11.87s (460/1000)] Perte : 2.3014\n",
            "[0m 12.12s (470/1000)] Perte : 2.8180\n",
            "[0m 12.36s (480/1000)] Perte : 2.8476\n",
            "[0m 12.74s (490/1000)] Perte : 3.4073\n",
            "[0m 13.11s (500/1000)] Perte : 2.3952\n",
            "[0m 13.47s (510/1000)] Perte : 3.3898\n",
            "[0m 13.83s (520/1000)] Perte : 3.3038\n",
            "[0m 14.20s (530/1000)] Perte : 2.5049\n",
            "[0m 14.59s (540/1000)] Perte : 3.4666\n",
            "[0m 14.94s (550/1000)] Perte : 2.7885\n",
            "[0m 15.18s (560/1000)] Perte : 2.7995\n",
            "[0m 15.42s (570/1000)] Perte : 3.5116\n",
            "[0m 15.65s (580/1000)] Perte : 3.3046\n",
            "[0m 15.89s (590/1000)] Perte : 3.1827\n",
            "[0m 16.12s (600/1000)] Perte : 3.4761\n",
            "[0m 16.36s (610/1000)] Perte : 2.6623\n",
            "[0m 16.59s (620/1000)] Perte : 2.9913\n",
            "[0m 16.83s (630/1000)] Perte : 3.6479\n",
            "[0m 17.06s (640/1000)] Perte : 3.1345\n",
            "[0m 17.31s (650/1000)] Perte : 2.3458\n",
            "[0m 17.54s (660/1000)] Perte : 3.5278\n",
            "[0m 17.78s (670/1000)] Perte : 2.9249\n",
            "[0m 18.01s (680/1000)] Perte : 2.7605\n",
            "[0m 18.25s (690/1000)] Perte : 3.2797\n",
            "[0m 18.48s (700/1000)] Perte : 3.1525\n",
            "[0m 18.72s (710/1000)] Perte : 2.6311\n",
            "[0m 18.96s (720/1000)] Perte : 3.1135\n",
            "[0m 19.19s (730/1000)] Perte : 3.4167\n",
            "[0m 19.43s (740/1000)] Perte : 3.2463\n",
            "[0m 19.67s (750/1000)] Perte : 4.4673\n",
            "[0m 19.91s (760/1000)] Perte : 3.1194\n",
            "[0m 20.17s (770/1000)] Perte : 3.5898\n",
            "[0m 20.43s (780/1000)] Perte : 2.7060\n",
            "[0m 20.67s (790/1000)] Perte : 3.2910\n",
            "[0m 20.90s (800/1000)] Perte : 3.2533\n",
            "[0m 21.14s (810/1000)] Perte : 2.7166\n",
            "[0m 21.38s (820/1000)] Perte : 2.5835\n",
            "[0m 21.61s (830/1000)] Perte : 2.9989\n",
            "[0m 21.85s (840/1000)] Perte : 3.0101\n",
            "[0m 22.09s (850/1000)] Perte : 2.8259\n",
            "[0m 22.34s (860/1000)] Perte : 2.4526\n",
            "[0m 22.58s (870/1000)] Perte : 2.3933\n",
            "[0m 22.81s (880/1000)] Perte : 3.5337\n",
            "[0m 23.05s (890/1000)] Perte : 2.6536\n",
            "[0m 23.28s (900/1000)] Perte : 3.1135\n",
            "[0m 23.53s (910/1000)] Perte : 3.4924\n",
            "[0m 23.77s (920/1000)] Perte : 2.8785\n",
            "[0m 24.00s (930/1000)] Perte : 3.0196\n",
            "[0m 24.23s (940/1000)] Perte : 3.8159\n",
            "[0m 24.48s (950/1000)] Perte : 3.2451\n",
            "[0m 24.71s (960/1000)] Perte : 3.5243\n",
            "[0m 24.98s (970/1000)] Perte : 3.1465\n",
            "[0m 25.34s (980/1000)] Perte : 2.6507\n",
            "[0m 25.71s (990/1000)] Perte : 3.5474\n",
            "[0m 26.06s (1000/1000)] Perte : 2.6123\n",
            "\n",
            "Évaluation...\n",
            "To be or not to beoladeeovons modre tor uts we dte asys. nd goole doo coir dylfon wontl ort wot f on odnnoolarokou ou net w otroty kore ws u ttttI rotn ood dookcl wfyris tonae an olaaylyuuol se yor on oyt turl we onuto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4oBGI42pxld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}